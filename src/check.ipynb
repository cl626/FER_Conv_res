{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import torch.hub\n",
    "import os\n",
    "import model\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from visualize.grad_cam import BackPropagation, GradCAM,GuidedBackPropagation\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('./visualize/haarcascade_frontalface_default.xml')\n",
    "shape = (48,48)\n",
    "classes = [\n",
    "    'Angry',\n",
    "    'Disgust',\n",
    "    'Fear',\n",
    "    'Happy',\n",
    "    'Sad',\n",
    "    'Surprised',\n",
    "    'Neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6667, 0.6863, 0.6000,  ..., 0.5608, 0.5725, 0.5608],\n",
       "          [0.6902, 0.7098, 0.2549,  ..., 0.5569, 0.5725, 0.5451],\n",
       "          [0.6314, 0.2824, 0.1608,  ..., 0.5451, 0.5608, 0.5333],\n",
       "          ...,\n",
       "          [0.1216, 0.0941, 0.1176,  ..., 0.0745, 0.1412, 0.2157],\n",
       "          [0.1569, 0.0980, 0.1059,  ..., 0.0863, 0.1490, 0.1804],\n",
       "          [0.2000, 0.0980, 0.0627,  ..., 0.0980, 0.1059, 0.1373]]]),\n",
       " array([[[140, 183, 186],\n",
       "         [153, 183, 188],\n",
       "         [143, 157, 162],\n",
       "         ...,\n",
       "         [129, 149, 149],\n",
       "         [133, 151, 150],\n",
       "         [125, 150, 152]],\n",
       " \n",
       "        [[148, 188, 190],\n",
       "         [166, 186, 191],\n",
       "         [ 60,  67,  69],\n",
       "         ...,\n",
       "         [128, 148, 149],\n",
       "         [133, 151, 150],\n",
       "         [123, 146, 148]],\n",
       " \n",
       "        [[136, 171, 173],\n",
       "         [ 65,  74,  81],\n",
       "         [ 42,  40,  41],\n",
       "         ...,\n",
       "         [125, 145, 146],\n",
       "         [130, 149, 147],\n",
       "         [121, 142, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 35,  29,  31],\n",
       "         [ 22,  23,  38],\n",
       "         [ 27,  29,  40],\n",
       "         ...,\n",
       "         [ 11,  19,  44],\n",
       "         [ 25,  36,  66],\n",
       "         [ 39,  55,  95]],\n",
       " \n",
       "        [[ 44,  39,  37],\n",
       "         [ 22,  25,  36],\n",
       "         [ 24,  27,  37],\n",
       "         ...,\n",
       "         [ 15,  21,  43],\n",
       "         [ 29,  38,  65],\n",
       "         [ 33,  46,  82]],\n",
       " \n",
       "        [[ 56,  50,  44],\n",
       "         [ 23,  25,  34],\n",
       "         [ 13,  16,  26],\n",
       "         ...,\n",
       "         [ 21,  24,  42],\n",
       "         [ 19,  27,  50],\n",
       "         [ 24,  35,  65]]], dtype=uint8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(image_path):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = cv2.imread(image_path)\n",
    "    # display(Image.fromarray(image))\n",
    "    #它使用了detectMultiScale函数来对输入的图像进行人脸检测，并返回一个包含检测到的人脸位置和大小的矩形框列表\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        image,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(1, 1),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        print('no face found')\n",
    "        face = cv2.resize(image, shape)\n",
    "    else:\n",
    "        (x, y, w, h) = faces[0]\n",
    "        face = image[y:y + h, x:x + w]\n",
    "        face = cv2.resize(face, shape)\n",
    "\n",
    "    img = Image.fromarray(face).convert('L')\n",
    "    # display(img)\n",
    "    inputs = transform_test(img)\n",
    "    return inputs, face\n",
    "\n",
    "preprocess(\"../test/sad.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度归一化为8bit uint\n",
    "def get_gradient_image(gradient):\n",
    "    gradient = gradient.cpu().numpy().transpose(1, 2, 0)    #轴转置\n",
    "    gradient -= gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    gradient *= 255.0\n",
    "    return np.uint8(gradient)\n",
    "\n",
    "\n",
    "# 图像融合\n",
    "def get_gradcam_image(gcam, raw_image, paper_cmap=False):\n",
    "    gcam = gcam.cpu().numpy()\n",
    "    cmap = cm.jet_r(gcam)[..., :3] * 255.0  #使用cm.jet_r函数将gcam映射为颜色映射（colormap），并将其乘以255.0以将值范围映射到0-255之间\n",
    "    if paper_cmap:\n",
    "        alpha = gcam[..., None]\n",
    "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "    else:\n",
    "        gcam = (cmap.astype(np.float64) + raw_image.astype(np.float64)) / 2\n",
    "    return np.uint8(gcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_backprop(images, model_name):\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        target, raw_image = preprocess(image['path'])\n",
    "        image['image'] = target\n",
    "        image['raw_image'] = raw_image\n",
    "\n",
    "    net = model.Model(num_classes=len(classes))\n",
    "    checkpoint = torch.load(os.path.join('../trained', model_name), map_location=torch.device('cpu'))\n",
    "    net.load_state_dict(checkpoint['net'])              #加载名为model_name的训练模型参数\n",
    "    net.eval()\n",
    "    summary(net, (1, shape[0], shape[1]))               #做网络的概述\n",
    "\n",
    "    result_images = []\n",
    "    for index, image in enumerate(images):              \n",
    "        img = torch.stack([image['image']])     \n",
    "        bp = BackPropagation(model=net)\n",
    "        probs, ids = bp.forward(img)\n",
    "        gcam = GradCAM(model=net)\n",
    "        _ = gcam.forward(img)\n",
    "\n",
    "        gbp = GuidedBackPropagation(model=net)\n",
    "        _ = gbp.forward(img)\n",
    "\n",
    "        # Guided Backpropagation\n",
    "        actual_emotion = ids[:,0]\n",
    "        gbp.backward(ids=actual_emotion.reshape(1,1))\n",
    "        gradients = gbp.generate()\n",
    "\n",
    "        # Grad-CAM\n",
    "        gcam.backward(ids=actual_emotion.reshape(1,1))\n",
    "        regions = gcam.generate(target_layer='last_conv')\n",
    "\n",
    "        # Get Images\n",
    "        label_image = np.zeros((shape[0],65, 3), np.uint8)\n",
    "        cv2.putText(label_image, classes[actual_emotion.data], (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # 把probs显示为概率文本\n",
    "        prob_image = np.zeros((shape[0],60,3), np.uint8)\n",
    "        cv2.putText(prob_image, '%.1f%%' % (probs.data[:,0] * 100), (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # 标准化图像(黑白)\n",
    "        guided_bpg_image = get_gradient_image(gradients[0])\n",
    "        guided_bpg_image = cv2.merge((guided_bpg_image, guided_bpg_image, guided_bpg_image))\n",
    "\n",
    "        # 融合图像，regions=最后一次卷积\n",
    "        grad_cam_image = get_gradcam_image(gcam=regions[0, 0],raw_image=image['raw_image'])\n",
    "\n",
    "        # 融合图像???\n",
    "        guided_gradcam_image = get_gradient_image(torch.mul(regions, gradients)[0])\n",
    "        guided_gradcam_image = cv2.merge((guided_gradcam_image, guided_gradcam_image, guided_gradcam_image))\n",
    "\n",
    "        img = cv2.hconcat([image['raw_image'],label_image,prob_image,guided_bpg_image,grad_cam_image,guided_gradcam_image])\n",
    "        result_images.append(img)\n",
    "        print(image['path'],classes[actual_emotion.data], probs.data[:,0] * 100)\n",
    "\n",
    "    cv2.imwrite('../test/guided_gradcam.jpg',cv2.resize(cv2.vconcat(result_images), None, fx=2,fy=2))\n",
    "\n",
    "\n",
    "def main():\n",
    "    guided_backprop(\n",
    "        images=[\n",
    "            {'path': '../test/angry.jpg'},\n",
    "            {'path': '../test/happy.jpg'},\n",
    "            {'path': '../test/sad.jpg'},\n",
    "            {'path': '../test/surprised.jpg'},\n",
    "        ],\n",
    "        model_name='private_model_233_66.t7'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
